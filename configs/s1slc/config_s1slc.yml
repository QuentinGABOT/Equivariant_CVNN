data:
  batch_size: 64
  dataset:
    name: S1SLC
    trainpath: ../../datasets/S1SLC
  #transform: LogAmplitude, Amplitude
  #transform: LogAmplitude, RealImaginary
  transform: LogAmplitude
  num_workers: 4
  valid_ratio: 0.15
  test_ratio: 0.15
logging:
  logdir: ./logs
  wandb:
    entity: equivariant_cvnn
    project: s1slc
loss:
  name: FocalLoss
  gamma: 5
model:
  #activation: ReLU
  activation: modReLU
  channels_ratio: 16
  class: ResNet
  res: true
  projection:
    #class: ModCtoR
    #class: NoCtoR
    class: PolyCtoR
    #class: MLPCtoR
    #global: false
    global: true
    #softmax: SoftmaxMeanCtoR
    #softmax: SoftmaxProductCtoR
    softmax: Softmax
  #downsampling: MaxPool
  #downsampling: StridedConv
  #downsampling: AvgPool
  downsampling: LPD
  #downsampling: APS
  #downsampling: LPF
  upsampling: Null
  latent_dim: Null
  dropout : 0.1
  normalization: 
    method: BatchNorm
    #method: Null
    #method: LayerNorm
    track_running_stats: false
  num_layers: 4
  gumbel_tau:
    start_value: 5
    gamma: 0.1
    start_decay_epoch: 5
    min_value: 0.00001
nepochs: 300
optim:
  algo: AdamW
  params:
    lr: 0.001
    weight_decay: 0.0005
scheduler:
  algo: StepLR
  params:
    step_size: 1000
    gamma: 1
  # algo: ReduceLROnPlateau
  # params:
  #   mode: min
  #   factor: 0.5
  #   patience: 5
  #   min_lr: 0.0005
  # algo: CyclicLR
  # params:
  #   base_lr: 0.0001
  #   max_lr: 0.001
  #   step_size_up: 100
  #   mode: triangular
  # algo: OneCycleLR
  # params:
  #   max_lr: 0.001
  # algo: CosineAnnealingLR
  # params:
  #   eta_min: 0.00005
  #algo: CosineAnnealingWarmRestarts
  #params:
    #T_0: 10
    #T_mult: 1
    #eta_min: 0.0001
pretrained: false
world_size: 4
dtype: complex64
#dtype: float64